python generate_flash.py \
    --use_flash_attention_2 \
    --model_path /root/autodl-tmp/dpo_new/outputs/llama-3-8b-instruct-simpo \
    --output_file alpaca_eval_outputs_simpo_8b_retrain.json \
    --batch_size 64
